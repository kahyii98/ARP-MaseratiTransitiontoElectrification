{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models to predict age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFilename = \"../all_posts_train.txt\"\n",
    "testFilename = \"../all_posts_test.txt\"\n",
    "\n",
    "# Define a mapping for the age groups to integer labels\n",
    "label_mapping = {10: 0, 20: 1, 30: 2, 40: 3}\n",
    "\n",
    "# Define a custom dataset class for handling the age group data\n",
    "class AgeGroupDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer  # Tokenizer for encoding the text\n",
    "        self.max_length = max_length  # Maximum sequence length\n",
    "        self.texts = []  # List to store text samples\n",
    "        self.labels = []  # List to store corresponding labels\n",
    "        \n",
    "        # Open the file and read line by line\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                words = line.rstrip().split()  # Split the line into words\n",
    "                category = int(words[0])  # Convert the first word to integer as category\n",
    "                words = words[1:]  # Rest of the words represent the text\n",
    "                # Join the words into a text string, considering only words with 3 parts (using word.split(\"/\"))\n",
    "                text = \" \".join([word.split(\"/\")[0] for word in words if len(word.split(\"/\")) == 3])\n",
    "                self.texts.append(text)  # Append the text to texts list\n",
    "                self.labels.append(label_mapping[category])  # Append the mapped label to labels list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)  # Return the total number of samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]  # Retrieve the text at index idx\n",
    "        label = self.labels[idx]  # Retrieve the corresponding label\n",
    "        # Encode the text using the tokenizer, truncating if necessary, and padding to max_length\n",
    "        encoding = self.tokenizer.encode_plus(text, truncation=True, max_length=self.max_length, padding='max_length')\n",
    "        # Return the encoding and label as a PyTorch tensor\n",
    "        return {key: torch.tensor(val) for key, val in encoding.items()}, torch.tensor(label)\n",
    "\n",
    "\n",
    "# Load the pre-trained BERT model for sequence classification.\n",
    "def prepare_model(num_labels):\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "    # The parameter \"bert-base-uncased\" specifies the pre-trained BERT model with uncased text processing.\n",
    "    # The num_labels parameter is used to specify the number of output classes (labels) for the classification task.\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=3):\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch, (inputs, labels) in tqdm(enumerate(train_loader)):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch, (inputs, labels) in tqdm(enumerate(val_loader)):\n",
    "                outputs = model(**inputs, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "        print(f\"Validation Loss: {val_loss / len(val_loader)}\")\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, labels) in tqdm(enumerate(test_loader)):\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy * 100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1614a49db4f343b09abb5b9f8e2ac138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fc7419d4d748198623b82d06b2e222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.087371581052182\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3420c2b62545ccb2b3da78769fd1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dfb5edd39b74035b9d7a0e9841382bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9527522752270896\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aada550747940a29eaf99105eecc564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71fa0a0e10994687bb81b51f256ad4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.827935048990701\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_dataset = AgeGroupDataset(trainFilename, tokenizer)\n",
    "test_dataset = AgeGroupDataset(testFilename, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "num_labels = len(set(train_dataset.labels))  # Number of unique age group categories\n",
    "model = prepare_model(num_labels)\n",
    "train_model(model, train_loader, train_loader)\n",
    "\n",
    "# Check accuracy\n",
    "evaluate_model(model, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54e86fa3c1f47479dbecb7bdac54e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.4%\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Increase Sequence Length <br>- Split Data into Training and Validation Sets <br>- Modify Hyperparameters <br>- Update the Training and Evaluation Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592da79d8a9e4f888d9804f53a5197d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fe4aa8714b4e44a3ec6784d4cdc9da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.134809607968611\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602fe4ac031743f696773abcf3869bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2befb74394b409091a9d44f3d5143d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0929678275304682\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c853e2ff698a46aca5ba00ac92f6fc0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b80dbffd0fe4c36bbe85fe784193129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0994654297828674\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e78fad8cd3474a8e1410f6fed5539c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df00d26b02b42338274c8cd6f263f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.1388374777401196\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd8784a908e433cad102e99cfa8f9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e905dcfc0564f108d4d0e59477c0757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.1777905278346117\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356f7c43f2c64b29a8eb4a31b0b87c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.4%\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "# Define file paths\n",
    "trainFilename = \"/Users/jocelyn/Desktop/ARP/all_posts_train.txt\"\n",
    "testFilename = \"/Users/jocelyn/Desktop/ARP/all_posts_test.txt\"\n",
    "\n",
    "# Define a mapping for the age groups\n",
    "label_mapping = {10: 0, 20: 1, 30: 2, 40: 3}\n",
    "\n",
    "# Define max_length\n",
    "max_length = 256\n",
    "\n",
    "# Define learning rate\n",
    "learning_rate = 2e-5\n",
    "\n",
    "class AgeGroupDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                words = line.rstrip().split()\n",
    "                category = int(words[0])\n",
    "                words = words[1:]\n",
    "                text = \" \".join([word.split(\"/\")[0] for word in words if len(word.split(\"/\")) == 3])\n",
    "                self.texts.append(text)\n",
    "                self.labels.append(label_mapping[category])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(text, truncation=True, max_length=self.max_length, padding='max_length')\n",
    "        return {key: torch.tensor(val) for key, val in encoding.items()}, torch.tensor(label)\n",
    "\n",
    "def prepare_model(num_labels):\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=5): # Increased epochs\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch, (inputs, labels) in tqdm(enumerate(train_loader)):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch, (inputs, labels) in tqdm(enumerate(val_loader)):\n",
    "                outputs = model(**inputs, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "        print(f\"Validation Loss: {val_loss / len(val_loader)}\")\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, labels) in tqdm(enumerate(test_loader)):\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy * 100}%\")\n",
    "\n",
    "\n",
    "# Load and tokenize data\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_dataset = AgeGroupDataset(trainFilename, tokenizer, max_length=max_length)\n",
    "test_dataset = AgeGroupDataset(testFilename, tokenizer, max_length=max_length)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Prepare and train the model\n",
    "num_labels = len(label_mapping)\n",
    "model = prepare_model(num_labels)\n",
    "train_model(model, train_loader, val_loader)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703b7c74bc3e44fb8118e596ea891413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153c66724ffa4a11947e1b3537ad11cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.1234658255296595\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586f73fa95064326a356cb97f2a36b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286cea4027f44b928b98a126ccacdfcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0545869890381308\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f399ab00cc2443c9bbb280f961f62b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a3fecf2d99441fb096bf2caa236196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0554057833026438\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15bfceca018841cc9700ea74a35ec0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79587422a4341ed80e697ff4177a2df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0862387629116284\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d08e0913bd415c90ad9838dc7cb87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cec2ff0183f43929d271259c82b217e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.171978308874018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a0f5933ab24e5fb9d4efd9ac9eb887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56.8%\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from transformers import BertConfig\n",
    "\n",
    "\n",
    "# Define file paths\n",
    "trainFilename = \"/Users/jocelyn/Desktop/ARP/all_posts_train.txt\"\n",
    "testFilename = \"/Users/jocelyn/Desktop/ARP/all_posts_test.txt\"\n",
    "\n",
    "# Define a mapping for the age groups\n",
    "label_mapping = {10: 0, 20: 1, 30: 2, 40: 3}\n",
    "\n",
    "# Define learning rate\n",
    "learning_rate = 2e-5\n",
    "\n",
    "class AgeGroupDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                words = line.rstrip().split()\n",
    "                category = int(words[0])\n",
    "                words = words[1:]\n",
    "                text = \" \".join([word.split(\"/\")[0] for word in words if len(word.split(\"/\")) == 3])\n",
    "                self.texts.append(text)\n",
    "                self.labels.append(label_mapping[category])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(text, truncation=True, max_length=self.max_length, padding='max_length')\n",
    "        return {key: torch.tensor(val) for key, val in encoding.items()}, torch.tensor(label)\n",
    "\n",
    "def prepare_model(num_labels, dropout_rate=0.1):\n",
    "    # Define the configuration with the specified dropout rate\n",
    "    config = BertConfig.from_pretrained(\"bert-base-uncased\", num_labels=num_labels, hidden_dropout_prob=dropout_rate)\n",
    "    # Load the model with the custom configuration\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", config=config)\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=5): # Increased epochs\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch, (inputs, labels) in tqdm(enumerate(train_loader)):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch, (inputs, labels) in tqdm(enumerate(val_loader)):\n",
    "                outputs = model(**inputs, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "        print(f\"Validation Loss: {val_loss / len(val_loader)}\")\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, labels) in tqdm(enumerate(test_loader)):\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy * 100}%\")\n",
    "\n",
    "\n",
    "# Load and tokenize data\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_dataset = AgeGroupDataset(trainFilename, tokenizer, max_length=max_length)\n",
    "test_dataset = AgeGroupDataset(testFilename, tokenizer, max_length=max_length)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Prepare and train the model\n",
    "num_labels = len(label_mapping)\n",
    "model = prepare_model(num_labels)\n",
    "train_model(model, train_loader, val_loader)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Roberta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ab887f4f1644ff84f4e372b2b9fe41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f719f768454b5cb33e30c0f7d504e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.198749642161762\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c654f11ded9d4b728f8c3ec0865d033f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9695b67ead044d329ba3a699084cd039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.1328245506567114\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38622ffb040e441e9b9d8526a077d792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9cfa31049b40c0a11f3ce51161516a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.147370994091034\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c2eb98521d4a06b63b01ab390db34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c1898b0e1024e3ea1bab5517b6fd232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.2195333926116718\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7febd2621274311aba1778b75b80068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7407ed28614142979a8cb678b022639a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.3643779018346\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1550fc9142b44195a1490369100d0893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.6%\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig\n",
    "\n",
    "\n",
    "# Define file paths\n",
    "trainFilename = \"/Users/jocelyn/Desktop/ARP/all_posts_train.txt\"\n",
    "testFilename = \"/Users/jocelyn/Desktop/ARP/all_posts_test.txt\"\n",
    "\n",
    "# Define a mapping for the age groups\n",
    "label_mapping = {10: 0, 20: 1, 30: 2, 40: 3}\n",
    "\n",
    "# Define learning rate\n",
    "learning_rate = 2e-5\n",
    "\n",
    "class AgeGroupDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                words = line.rstrip().split()\n",
    "                category = int(words[0])\n",
    "                words = words[1:]\n",
    "                text = \" \".join([word.split(\"/\")[0] for word in words if len(word.split(\"/\")) == 3])\n",
    "                self.texts.append(text)\n",
    "                self.labels.append(label_mapping[category])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(text, truncation=True, max_length=self.max_length, padding='max_length')\n",
    "        return {key: torch.tensor(val) for key, val in encoding.items()}, torch.tensor(label)\n",
    "\n",
    "def prepare_model(num_labels, dropout_rate=0.1):\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=num_labels)\n",
    "    return model\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, train_loader, val_loader, epochs=5, class_weights=None):\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    loss_function = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch, (inputs, labels) in tqdm(enumerate(train_loader)):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_function(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch, (inputs, labels) in tqdm(enumerate(val_loader)):\n",
    "                outputs = model(**inputs, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "        print(f\"Validation Loss: {val_loss / len(val_loader)}\")\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, labels) in tqdm(enumerate(test_loader)):\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy * 100}%\")\n",
    "\n",
    "\n",
    "# Load and tokenize data\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "train_dataset = AgeGroupDataset(trainFilename, tokenizer, max_length=max_length)\n",
    "test_dataset = AgeGroupDataset(testFilename, tokenizer, max_length=max_length)\n",
    "\n",
    "# Split the training data into training and validation subsets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Calculate class weights (you should implement this based on your data distribution)\n",
    "class_weights = None  # Replace with the computed class weights\n",
    "\n",
    "# Prepare and train the model\n",
    "num_labels = len(label_mapping)\n",
    "model = prepare_model(num_labels)\n",
    "train_model(model, train_loader, val_loader, class_weights=class_weights)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Roberta\n",
    "\n",
    "Calculate Class Weights and use it in Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3cb90c14f34e40885754c8a277afeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8815262436b44dfca09df9063f13f15e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.1751519161112167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154e10e088f145ea855bfb4dc4b57d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1780d2a4c3394cf38186391e10a1deab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.1696149864617515\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201e81f0eb1042af88ed0e62fb0038af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4a7c02504a4cc6bbe014e55fb55200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.1680370937375462\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90ac8243f3a47d79ff5534f30f7f4b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4dbe078b434cf6a66150910692fe1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.1454929989926956\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43463062ca114902af1218166c432f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ad43c9cad2415490d41c725b20a43e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.2326692430412067\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0864a4c06aff456f81d6e3c9b1dc4a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 57.199999999999996%\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "from transformers import BertConfig\n",
    "\n",
    "\n",
    "# Define file paths\n",
    "trainFilename = \"/Users/jocelyn/Desktop/ARP/all_posts_train.txt\"\n",
    "testFilename = \"/Users/jocelyn/Desktop/ARP/all_posts_test.txt\"\n",
    "\n",
    "# Define a mapping for the age groups\n",
    "label_mapping = {10: 0, 20: 1, 30: 2, 40: 3}\n",
    "\n",
    "max_length = 128 \n",
    "\n",
    "# Define learning rate\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# AgeGroupDataset class\n",
    "class AgeGroupDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                words = line.rstrip().split()\n",
    "                category = int(words[0])\n",
    "                words = words[1:]\n",
    "                text = \" \".join([word.split(\"/\")[0] for word in words if len(word.split(\"/\")) == 3])\n",
    "                self.texts.append(text)\n",
    "                self.labels.append(label_mapping[category])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(text, truncation=True, max_length=self.max_length, padding='max_length')\n",
    "        return {key: torch.tensor(val) for key, val in encoding.items()}, torch.tensor(label)\n",
    "\n",
    "\n",
    "# Calculate class weights\n",
    "label_counter = Counter(train_dataset.labels)\n",
    "total_samples = len(train_dataset.labels)\n",
    "class_weights = {k: total_samples / (len(label_mapping) * v) for k, v in label_counter.items()}\n",
    "class_weights_tensor = torch.tensor([class_weights[i] for i in range(len(class_weights))], dtype=torch.float)\n",
    "\n",
    "def prepare_model(num_labels, dropout_rate=0.1):\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=num_labels)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, train_loader, val_loader, epochs=5):\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    loss_function = nn.CrossEntropyLoss(weight=class_weights_tensor)  # Using class_weights_tensor directly\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch, (inputs, labels) in tqdm(enumerate(train_loader)):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_function(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch, (inputs, labels) in tqdm(enumerate(val_loader)):\n",
    "                outputs = model(**inputs, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "        print(f\"Validation Loss: {val_loss / len(val_loader)}\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, labels) in tqdm(enumerate(test_loader)):\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy * 100}%\")\n",
    "\n",
    "\n",
    "# Load and tokenize data\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "train_dataset = AgeGroupDataset(trainFilename, tokenizer, max_length=max_length)\n",
    "test_dataset = AgeGroupDataset(testFilename, tokenizer, max_length=max_length)\n",
    "\n",
    "# Split the training data into training and validation subsets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Prepare and train the model\n",
    "num_labels = len(label_mapping)\n",
    "model = prepare_model(num_labels)\n",
    "train_model(model, train_loader, val_loader)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "evaluate_model(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa7b63b2816c1bdbcd645d26c2b2c86c1f9533463077c1036b1b552d7aaa1234"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
