{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model code is provided by https://github.com/twistedTightly/NLP-Age-Classification/blob/master/src/pos_tagger_class.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 2: Part of Speech tagging using bigrams\n",
    "\n",
    "import collections, math\n",
    "\n",
    "class pos_tagger:\n",
    "\n",
    "\t#Training\n",
    "\tdef __init__(self, trainData):\n",
    "\t\t#Initialize data structures\n",
    "\t\ttagBigramCounts = dict()\n",
    "\t\ttagBigramCounts[\"<s>\"] = collections.Counter()\n",
    "\t\ttagWordCounts = dict()\n",
    "\t\tvocab = set()\n",
    "\t\ttagVocab = set()\n",
    "\t\ttagBigramProbs = dict()\n",
    "\t\ttagWordProbs = dict()\n",
    "\n",
    "\t\t#Work through the training data and count tag bigrams and word-tag pairs\n",
    "\t\tnumWords = 0\n",
    "\t\tfor line in trainData:\n",
    "\t\t\tprevTag = \"<s>\"\n",
    "\t\t\twords = line.split()[1:]\n",
    "\t\t\tage = words[0]\n",
    "\t\t\twords = words[1:]\n",
    "\n",
    "\t\t\tfor wordTagPair in words:\n",
    "\t\t\t\tnumWords += 1\n",
    "\t\t\t\twordTagPair = wordTagPair.split('/')\n",
    "\t\t\n",
    "\t\t\t\tword = wordTagPair[0]\n",
    "\t\t\t\ttag = wordTagPair[1]\n",
    "\n",
    "\t\t\t\t#Initialize the counter if the tag hasn't been seen before\n",
    "\t\t\t\tif tag not in tagWordCounts:\n",
    "\t\t\t\t\ttagWordCounts[tag] = collections.Counter()\n",
    "\t\t\t\tif tag not in tagBigramCounts: #Prep for the next tag we'll see\n",
    "\t\t\t\t\ttagBigramCounts[tag] = collections.Counter()\n",
    "\t\t\t\t\ttagVocab.add(tag)\n",
    "\t\t\t\tif word not in vocab:\n",
    "\t\t\t\t\tvocab.add(word)\n",
    "\t\t\n",
    "\t\t\t\t#Add to tagBigramCounts for prevTag-tag bigram\n",
    "\t\t\t\ttagBigramCounts[prevTag][tag] += 1\n",
    "\n",
    "\t\t\t\t#Add to tagWordCounts\n",
    "\t\t\t\ttagWordCounts[tag][word] += 1\n",
    "\n",
    "\t\t\t\t#Set current tag as new prevTag\n",
    "\t\t\t\tprevTag = tag\n",
    "\t\n",
    "\t\t\t#Account for end of line symbol </s>\n",
    "\t\t\ttagBigramCounts[prevTag][\"</s>\"] += 1\n",
    "\n",
    "\t\t#Get vocabulary size\n",
    "\t\tvocabSize = len(vocab)\n",
    "\t\ttagVocabSize = len(tagVocab)\n",
    "\t\ttagVocabWithEnd = set(tagVocab)\n",
    "\t\ttagVocabWithEnd.add(\"</s>\") #We only need to check the end tag sometimes\n",
    "\n",
    "\t\t#Set smoothing parameter\n",
    "\t\teta = 0.005\n",
    "\n",
    "\t\t#Get probabilities of each tag bigram\n",
    "\t\tfor sourceTag in tagBigramCounts:\n",
    "\t\t\tthisTagTotal = sum(tagBigramCounts[sourceTag].values())\n",
    "\t\t\ttagBigramProbs[sourceTag] = dict()\n",
    "\t\t\t#for destinationTag in tagBigramCounts[sourceTag]:\n",
    "\t\t\tfor destinationTag in tagVocabWithEnd:\n",
    "\t\t\t\tif destinationTag in tagBigramCounts[sourceTag]:\n",
    "\t\t\t\t\ttagBigramProbs[sourceTag][destinationTag] = float(tagBigramCounts[sourceTag][destinationTag] + eta)/(thisTagTotal + tagVocabSize)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttagBigramProbs[sourceTag][destinationTag] = eta/(thisTagTotal + tagVocabSize)\n",
    "\n",
    "\t\t#Get p(w|t) probabilities\n",
    "\t\tfor tag in tagWordCounts:\n",
    "\t\t\tthisTagTotal = sum(tagWordCounts[tag].values())\n",
    "\t\t\ttagWordProbs[tag] = dict()\n",
    "\t\t\tfor word in tagWordCounts[tag]:\n",
    "\t\t\t\ttagWordProbs[tag][word] = float(tagWordCounts[tag][word] + eta)/(thisTagTotal + vocabSize)\n",
    "\n",
    "\n",
    "\t\t#Save relevant data structures\n",
    "\t\tself.tagBigramProbs = tagBigramProbs\n",
    "\t\tself.tagWordProbs = tagWordProbs\n",
    "\t\tself.tagVocab = tagVocab\n",
    "\t\tself.vocabSize = vocabSize\n",
    "\n",
    "\t#Testing\n",
    "\tdef decode(self, words):\n",
    "\t\tnumWords = len(words)\n",
    "\n",
    "\t\t#Get relevant data structures\n",
    "\t\ttagBigramProbs = self.tagBigramProbs\n",
    "\t\ttagWordProbs = self.tagWordProbs\n",
    "\t\ttagVocab = self.tagVocab\n",
    "\t\tvocabSize = self.vocabSize\n",
    "\n",
    "\t\t#Create array of tag names; these will become state names\n",
    "\t\ttags = []\n",
    "\t\tfor tag in tagVocab:\n",
    "\t\t\ttags.append(tag)\n",
    "\n",
    "\t\t#Create list of states- this is a two-dimensional array that represents the states of the HMM. Traverse each row to get topological order\n",
    "\t\tstates = []\n",
    "\t\tstates.append([\"<s>\"])\n",
    "\t\tfor i in range(1, numWords+1):\n",
    "\t\t\tstates.append(tags)\n",
    "\t\tstates.append([\"</s>\"])\n",
    "\n",
    "\t\t#Initialize viterbi data structures. viterbi and pointers are arrays of dictionaries, where each row represents a word in the word sequence \n",
    "\t\t#and the dictionary keys are the different states (possible tags). For viterbi, the values are the log weights; for pointers, the values \n",
    "\t\t#are the previous states\n",
    "\t\tviterbi = [] \n",
    "\t\tpointers = []\n",
    "\t\tviterbi.append({\"<s>\": 0})\n",
    "\t\tpointers.append({\"<s>\": 0})\n",
    "\t\tfor i in range(1, numWords+1):\n",
    "\t\t\tviterbi.append(dict())\n",
    "\t\t\tpointers.append(dict())\n",
    "\t\t\tfor tag in tags:\n",
    "\t\t\t\tviterbi[i][tag] = 0\n",
    "\t\t\t\tpointers[i][tag] = 0\n",
    "\t\tviterbi.append({\"</s>\": 0})\n",
    "\t\tpointers.append({\"</s>\": 0})\n",
    "\t\n",
    "\t\t#Work through the Viterbi algorithm\n",
    "\t\tfor i in range(1, numWords+1): #Work through each word in the word sequence\n",
    "\t\t\twordTagPair = words[i-1].split('/')\n",
    "\t\t\tword = wordTagPair[0]\n",
    "\t\t\tactualTag = wordTagPair[1]\n",
    "\n",
    "\t\t\tfor destinationState in states[i]: #For each possible tag\n",
    "\t\t\t\tfor sourceState in states[i-1]: #Loop through each transition to this tag\n",
    "\t\t\t\t\tif word in tagWordProbs[destinationState]:\n",
    "\t\t\t\t\t\t#The log probability is log( p(t'|t) * p(w|t) )\n",
    "\t\t\t\t\t\tlogProb = math.log10(tagBigramProbs[sourceState][destinationState]*tagWordProbs[destinationState][word])\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tlogProb = math.log10(tagBigramProbs[sourceState][destinationState]*0.001/float(vocabSize)) #Give default value- unknown word\n",
    "\t\t\t\t\tnewWeight = viterbi[i-1][sourceState] + logProb #Add to current running log weight\n",
    "\t\t\t\t\tif newWeight > viterbi[i][destinationState] or viterbi[i][destinationState] == 0: #If we find a new best weight or it hasn't been set yet\n",
    "\t\t\t\t\t\tviterbi[i][destinationState] = newWeight\n",
    "\t\t\t\t\t\tpointers[i][destinationState] = sourceState\n",
    "\n",
    "\t\t#Do final state: \n",
    "\t\tdestinationState = \"</s>\"\n",
    "\t\tfor sourceState in states[numWords]:\n",
    "\t\t\tlogProb = math.log10(tagBigramProbs[sourceState][destinationState])\n",
    "\t\t\tnewWeight = viterbi[numWords][sourceState] + logProb\n",
    "\t\t\tif newWeight > viterbi[numWords+1][destinationState] or viterbi[numWords+1][destinationState] == 0:\n",
    "\t\t\t\tviterbi[numWords+1][destinationState] = newWeight\n",
    "\t\t\t\tpointers[numWords+1][destinationState] = sourceState\n",
    "\n",
    "\t\t#Reconstruct tag sequence from pointers\n",
    "\t\ttagSequence = [0 for i in range(0, numWords + 1)]\n",
    "\t\ti = numWords\n",
    "\t\ttagSequence[numWords] = \"</s>\"\n",
    "\t\twhile i > 0:\n",
    "\t\t\ttagSequence[i-1] = pointers[i+1][tagSequence[i]]\n",
    "\t\t\ti = i-1\n",
    "\n",
    "\t\treturn tagSequence\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('DigitalTech')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a198fc3a2683bd958433f4d7e447d5eeb9d1c4c7374695088b76366538a7df0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
